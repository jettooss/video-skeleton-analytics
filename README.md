## Аналитика и сравнение моделей обработки видео

### 1. Введение

В данном исследовании рассматриваются методы классификации спортивных движений человека на видео (например, отжимания и приседания). Основная цель — сравнить эффективность различных подходов и моделей по качеству распознавания и времени обучения.

## Структура и назначение файлов

* **video\_segmenter.ipynb** — подготовка датасета: автоматическая нарезка видео на сегменты по движению рук («спуск»/«подъём») с помощью модели Wholebody из rtmlib, аугментация (отражение, сдвиг) и сохранение.

* **to\_npy.ipynb** — извлечение 17 ключевых точек из каждого кадра с помощью модели Body из rtmlib, сохранение данных в формате .npy и формирование CSV-таблиц для обучения и валидации.

* **ml\_model\_yolo.ipynb** —  тестирование yolo pose-модели для классификации скелета.

* **hybrid\_gcn\_timesformer\_pushup\_joint\_dropout.ipynb**, **hybrid\_gcn\_timesformer\_pushup.ipynb**, **train\_k400.ipynb** — эксперименты с гибридными архитектурами.

###  Детали работы video\_segmenter.ipynb

Используются модели из rtmlib для позевой оценки, поскольку они обеспечивают более точное и устойчивое определение скелетных ключевых точек, что позволяет превосходить yolo pose в классификации фаз движения.

\- Видео разбивается на сегменты по 8 кадров.

\- Для каждого сегмента вычисляются углы в локтях:

&#x20;   \- <130° — метка "спуск"

&#x20;   \- >160° — метка "подъем"

\- Сегменты сохраняются с меткой в имени файла, дополнительно создаются аугментированные версии (отражение, сдвиг).

\- Для категории "man\_doing\_something" видео просто разбивается на сегменты без вычисления углов и аугментации.

## Детали работы to\_npy.ipynb

\- Для каждого видео извлекается последовательность ключевых точек (по умолчанию 17 точек на 4 кадра).

\- Ключевые точки сохраняются в .npy-файлы, а информация о каждом видео и его метке — в csv-таблицы (train/val).

\- Обработка происходит параллельно для ускорения (ThreadPoolExecutor).

\- Поддерживаются классы: "false", "man\_doing\_something", "true"

### Кратко о hybrid\_gcn\_timesformer\_pushup

Базовая гибридная модель сочетает графовую сверточную сеть (GCN) для учёта пространственных связей между суставами и архитектуру Timesformer для моделирования временной динамики движения. Модель обрабатывает последовательность скелетных ключевых точек длиной 4 кадра, что обеспечивает оптимальный баланс между детализацией информации и вычислительной производительностью. Благодаря такому комбинированному подходу модель эффективно захватывает как локальные, так и глобальные паттерны движений, что обеспечивает высокую точность классификации отжиманий при умеренном времени обучения.

### Различия между hybrid\_gcn\_timesformer\_pushup и hybrid\_gcn\_timesformer\_pushup\_joint\_dropout

* В экспериментах модель с `RandomJointDropout` демонстрировала прирост метрик на 5–6% по сравнению с базовой гибридной архитектурой.

* **hybrid\_gcn\_timesformer\_pushup.ipynb**: базовая гибридная архитектура GCN + Timesformer для классификации отжиманий без регуляризации.

* **hybrid\_gcn\_timesformer\_pushup\_joint\_dropout.ipynb**: добавляет слой `RandomJointDropout`, который во время обучения с вероятностью p обнуляет отдельные суставы (по оси V) в тензоре входных данных (B, T, V, C), что повышает устойчивость модели к шуму и снижает переобучение.

Пример реализации `RandomJointDropout`:

```python
class RandomJointDropout(nn.Module):
    def __init__(self, p: float = 0.1):
        """
        p — вероятность обнуления каждого сустава.
        """
        super().__init__()
        self.p = p

    def forward(self, x):
        # x: (B, T, V, C)
        if not self.training or self.p == 0.0:
            return x
        B, T, V, C = x.shape
        mask = (torch.rand(B, V, device=x.device) > self.p) \
               .float() \
               .view(B, 1, V, 1)
        return x * mask
```

### Детали работы ml\_model\_yolo.ipynb

> **Упор на быстроту:** данный подход оптимизирован для максимально быстрого извлечения ключевых точек и вычисления признаков в режиме почти реального времени.

В этом ноутбуке используется модель YOLO-pose (`yolo11x-pose.pt`) для извлечения 17 ключевых точек позы из каждого кадра. После получения координат скелета вычисляются геометрические признаки:

* Евклидовы расстояния между ключевыми точками по заданным соединениям (`skeleton`).
* Углы в ключевых суставах по тройкам точек.

Данные объединяются в `pandas.DataFrame`, где каждая строка соответствует одному кадру и содержит:

* Название видео и номер кадра.
* Координаты ключевых точек `kpX_x`, `kpX_y`.
* Рассчитанные признаки `distance_i_j` и `angle_i_j_k`.
* Метку класса упражнения.

На основе сформированного датасета обучаются классические модели (`Logistic Regression`, `Decision Tree`, `Random Forest`, `SVM`, `KNN`, `Gradient Boosting`, `XGBoost`, `Neural Network`). Качество оценки проводится по метрикам `Accuracy`, `Precision`, `Recall` и `F1-Score`.

| Модель                 | Accuracy | Precision | Recall | F1-Score |
| ---------------------- | -------: | --------: | -----: | -------: |
| Logistic Regression    |   0.7301 |    0.7094 | 0.7236 |   0.7158 |
| Decision Tree          |   0.5199 |    0.5151 | 0.5712 |   0.5212 |
| Random Forest          |   0.6881 |    0.6639 | 0.7096 |   0.6776 |
| Support Vector Machine |   0.4646 |    0.3424 | 0.4911 |   0.3477 |
| K-Nearest Neighbors    |   0.5719 |    0.5293 | 0.5613 |   0.5314 |
| Gradient Boosting      |   0.6925 |    0.6519 | 0.6629 |   0.6561 |
| XGBoost                |   0.7323 |    0.7033 | 0.7175 |   0.7071 |
| Neural Network         |   0.4878 |    0.7050 | 0.6117 |   0.5434 |

## Описание модели `facebook/timesformer-base-finetuned-k400`

**Название:** `facebook/timesformer-base-finetuned-k400`
**Архитектура:** TimeSformer Base (разделённое пространственно-временное внимание)

### Основная суть модели

TimeSformer адаптирует концепцию Vision Transformer (ViT) к видеофайлам, разбивая каждый кадр на патчи (обычно 16×16 пикселей) и применяя разделённый механизм внимания:

* **Пространственное внимание** внутри каждого кадра позволяет модели улавливать локальные визуальные паттерны.
* **Временное внимание** между кадрами обеспечивает учёт динамики и изменений движения во времени.

**Выводы**  `timesformer-base-finetuned-k400`:Неэффективность на текущих данных: модель демонстрирует сильное переобучение (высокая точность на тренировке при низкой на валидации).

* **Высокая вычислительная нагрузка:** даже при успешном обучении модель слишком тяжёлая и требует значительных ресурсов, что снижает её практическую применимость.
* **Медленная обработка:** скорость обучения и инференса недостаточна для реального времени или массовой обработки видео.

3. Сравнение результатов

| Модель                                                 | Val Loss | Val Acc (Quality) | Overall F1 (Quality) |
| ------------------------------------------------------ | -------- | ----------------- | -------------------- |
| hybrid\_gcn\_timesformer\_pushup\_joint\_dropout       | 0.1576   | 0.9162            | 0.9116               |
| hybrid\_gcn\_timesformer\_pushup                       | 0.2203   | 0.8586            | 0.8466               |
| facebook/timesformer-base-finetuned-k400 (train\_k400) | 1.2287   | 0.4635            | 0.4643               |

**Ключевые наблюдения:**

* Лучшие результаты показала модель с `RandomJointDropout`: Val F1 ≈ 0.91.
* Базовая гибридная модель уступает ей на \~6.5% по F1.
* Предобученный TimeSformer (`train_k400`) демонстрирует низкую эффективность на специализированных отжиманиях, с Val F1 ≈ 0.46.

---

## Результаты классических ML-моделей по F1-Score

| Модель                 | F1-Score |
| ---------------------- | -------: |
| XGBoost                |   0.7071 |
| Logistic Regression    |   0.7158 |
| Random Forest          |   0.6776 |
| Gradient Boosting      |   0.6561 |
| K-Nearest Neighbors    |   0.5314 |
| Support Vector Machine |   0.3477 |
| Neural Network         |   0.5434 |
| Decision Tree          |   0.5212 |

---

## Общий вывод

На основе проведённого исследования можно сделать следующие выводы:

1. **Лучший компромисс точность/устойчивость** обеспечивает гибридная модель `hybrid_gcn_timesformer_pushup_joint_dropout` с RandomJointDropout (Val F1 ≈ 0.91).
2. **Быстрый фреймворк YOLO-pose** и классические ML-модели показывают средние результаты (F1 \~0.53–0.73), при этом они просты в использовании и менее требовательны к ресурсам.
3. **Предобученный TimeSformer** (`facebook/timesformer-base-finetuned-k400`) не справляется с задачей распознавания специализированных упражнений (Val F1 ≈ 0.46) и требует доработки датасета или тонкой настройки гиперпараметров.
4. **Практическая рекомендация**: для промышленного внедрения стоит использовать гибридные архитектуры с dropout-регуляризацией, а при ограниченных ресурсах — ML-модели или YOLO-pose для быстрого прототипирования.

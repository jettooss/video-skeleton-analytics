{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d02a89-3a9e-47ae-bdf7-ed3cea274cf2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load C:\\Users\\jet\\.cache\\rtmlib\\hub\\checkpoints\\yolox_tiny_8xb8-300e_humanart-6f3252f9.onnx with onnxruntime backend\n",
      "load C:\\Users\\jet\\.cache\\rtmlib\\hub\\checkpoints\\rtmpose-s_simcc-body7_pt-body7_420e-256x192-acd4a1ef_20230504.onnx with onnxruntime backend\n",
      "extract_skeletons: 0.0849s\n",
      "predict_segment_onnx: 0.0859s\n",
      "extract_skeletons: 0.0843s\n",
      "predict_segment_onnx: 0.0853s\n",
      "extract_skeletons: 0.0813s\n",
      "predict_segment_onnx: 0.0839s\n",
      "extract_skeletons: 0.0801s\n",
      "predict_segment_onnx: 0.0811s\n",
      "extract_skeletons: 0.0796s\n",
      "predict_segment_onnx: 0.0821s\n",
      "extract_skeletons: 0.0808s\n",
      "predict_segment_onnx: 0.0828s\n",
      "extract_skeletons: 0.1008s\n",
      "predict_segment_onnx: 0.1018s\n",
      "extract_skeletons: 0.0940s\n",
      "predict_segment_onnx: 0.0960s\n",
      "extract_skeletons: 0.0897s\n",
      "predict_segment_onnx: 0.0907s\n",
      "extract_skeletons: 0.0818s\n",
      "predict_segment_onnx: 0.0838s\n",
      "extract_skeletons: 0.1117s\n",
      "predict_segment_onnx: 0.1137s\n",
      "extract_skeletons: 0.0840s\n",
      "predict_segment_onnx: 0.0862s\n",
      "extract_skeletons: 0.0923s\n",
      "predict_segment_onnx: 0.0943s\n",
      "extract_skeletons: 0.0805s\n",
      "predict_segment_onnx: 0.0825s\n",
      "extract_skeletons: 0.0818s\n",
      "predict_segment_onnx: 0.0837s\n",
      "extract_skeletons: 0.0832s\n",
      "predict_segment_onnx: 0.0852s\n",
      "extract_skeletons: 0.0841s\n",
      "predict_segment_onnx: 0.0851s\n",
      "extract_skeletons: 0.0812s\n",
      "predict_segment_onnx: 0.0833s\n",
      "extract_skeletons: 0.0909s\n",
      "predict_segment_onnx: 0.0929s\n",
      "extract_skeletons: 0.0784s\n",
      "predict_segment_onnx: 0.0804s\n",
      "extract_skeletons: 0.0821s\n",
      "predict_segment_onnx: 0.0831s\n",
      "extract_skeletons: 0.0850s\n",
      "predict_segment_onnx: 0.0871s\n",
      "extract_skeletons: 0.0868s\n",
      "predict_segment_onnx: 0.0883s\n",
      "extract_skeletons: 0.0849s\n",
      "predict_segment_onnx: 0.0859s\n",
      "extract_skeletons: 0.0863s\n",
      "predict_segment_onnx: 0.0877s\n",
      "extract_skeletons: 0.0901s\n",
      "predict_segment_onnx: 0.0917s\n",
      "extract_skeletons: 0.0851s\n",
      "predict_segment_onnx: 0.0871s\n",
      "extract_skeletons: 0.0910s\n",
      "predict_segment_onnx: 0.0925s\n",
      "extract_skeletons: 0.0849s\n",
      "predict_segment_onnx: 0.0874s\n",
      "extract_skeletons: 0.0787s\n",
      "predict_segment_onnx: 0.0797s\n",
      "extract_skeletons: 0.0779s\n",
      "predict_segment_onnx: 0.0789s\n",
      "extract_skeletons: 0.0831s\n",
      "predict_segment_onnx: 0.0851s\n",
      "extract_skeletons: 0.0760s\n",
      "predict_segment_onnx: 0.0770s\n",
      "extract_skeletons: 0.0789s\n",
      "predict_segment_onnx: 0.0799s\n",
      "extract_skeletons: 0.0778s\n",
      "predict_segment_onnx: 0.0788s\n",
      "extract_skeletons: 0.0781s\n",
      "predict_segment_onnx: 0.0801s\n",
      "extract_skeletons: 0.0789s\n",
      "predict_segment_onnx: 0.0805s\n",
      "extract_skeletons: 0.0793s\n",
      "predict_segment_onnx: 0.0813s\n",
      "extract_skeletons: 0.0829s\n",
      "predict_segment_onnx: 0.0849s\n",
      "extract_skeletons: 0.0768s\n",
      "predict_segment_onnx: 0.0788s\n",
      "extract_skeletons: 0.0822s\n",
      "predict_segment_onnx: 0.0842s\n",
      "extract_skeletons: 0.0799s\n",
      "predict_segment_onnx: 0.0814s\n",
      "extract_skeletons: 0.0808s\n",
      "predict_segment_onnx: 0.0819s\n",
      "extract_skeletons: 0.0797s\n",
      "predict_segment_onnx: 0.0812s\n",
      "extract_skeletons: 0.0787s\n",
      "predict_segment_onnx: 0.0797s\n",
      "extract_skeletons: 0.0770s\n",
      "predict_segment_onnx: 0.0791s\n",
      "extract_skeletons: 0.0853s\n",
      "predict_segment_onnx: 0.0868s\n",
      "extract_skeletons: 0.1286s\n",
      "predict_segment_onnx: 0.1306s\n",
      "extract_skeletons: 0.0779s\n",
      "predict_segment_onnx: 0.0799s\n",
      "extract_skeletons: 0.0775s\n",
      "predict_segment_onnx: 0.0795s\n",
      "extract_skeletons: 0.0815s\n",
      "predict_segment_onnx: 0.0825s\n",
      "extract_skeletons: 0.0805s\n",
      "predict_segment_onnx: 0.0820s\n",
      "extract_skeletons: 0.0891s\n",
      "predict_segment_onnx: 0.0907s\n",
      "extract_skeletons: 0.0784s\n",
      "predict_segment_onnx: 0.0793s\n",
      "extract_skeletons: 0.0787s\n",
      "predict_segment_onnx: 0.0797s\n",
      "extract_skeletons: 0.0774s\n",
      "predict_segment_onnx: 0.0794s\n",
      "extract_skeletons: 0.0780s\n",
      "predict_segment_onnx: 0.0810s\n",
      "extract_skeletons: 0.0840s\n",
      "predict_segment_onnx: 0.0861s\n",
      "extract_skeletons: 0.0803s\n",
      "predict_segment_onnx: 0.0813s\n",
      "extract_skeletons: 0.0781s\n",
      "predict_segment_onnx: 0.0801s\n",
      "extract_skeletons: 0.1080s\n",
      "predict_segment_onnx: 0.1100s\n",
      "extract_skeletons: 0.0862s\n",
      "predict_segment_onnx: 0.0872s\n",
      "extract_skeletons: 0.0893s\n",
      "predict_segment_onnx: 0.0903s\n",
      "extract_skeletons: 0.0789s\n",
      "predict_segment_onnx: 0.0809s\n",
      "extract_skeletons: 0.0822s\n",
      "predict_segment_onnx: 0.0842s\n",
      "extract_skeletons: 0.0756s\n",
      "predict_segment_onnx: 0.0781s\n",
      "extract_skeletons: 0.0779s\n",
      "predict_segment_onnx: 0.0789s\n",
      "extract_skeletons: 0.0786s\n",
      "predict_segment_onnx: 0.0801s\n",
      "extract_skeletons: 0.0791s\n",
      "predict_segment_onnx: 0.0801s\n",
      "extract_skeletons: 0.0771s\n",
      "predict_segment_onnx: 0.0781s\n",
      "extract_skeletons: 0.0764s\n",
      "predict_segment_onnx: 0.0779s\n",
      "extract_skeletons: 0.0803s\n",
      "predict_segment_onnx: 0.0823s\n",
      "extract_skeletons: 0.0797s\n",
      "predict_segment_onnx: 0.0808s\n",
      "extract_skeletons: 0.0942s\n",
      "predict_segment_onnx: 0.0957s\n",
      "extract_skeletons: 0.0803s\n",
      "predict_segment_onnx: 0.0813s\n",
      "extract_skeletons: 0.0779s\n",
      "predict_segment_onnx: 0.0789s\n",
      "extract_skeletons: 0.0814s\n",
      "predict_segment_onnx: 0.0827s\n",
      "extract_skeletons: 0.0806s\n",
      "predict_segment_onnx: 0.0816s\n",
      "extract_skeletons: 0.0767s\n",
      "predict_segment_onnx: 0.0788s\n",
      "extract_skeletons: 0.0796s\n",
      "predict_segment_onnx: 0.0811s\n",
      "extract_skeletons: 0.0870s\n",
      "predict_segment_onnx: 0.0879s\n",
      "extract_skeletons: 0.0773s\n",
      "predict_segment_onnx: 0.0783s\n",
      "extract_skeletons: 0.0901s\n",
      "predict_segment_onnx: 0.0917s\n",
      "extract_skeletons: 0.0813s\n",
      "predict_segment_onnx: 0.0833s\n",
      "extract_skeletons: 0.0791s\n",
      "predict_segment_onnx: 0.0813s\n",
      "extract_skeletons: 0.0850s\n",
      "predict_segment_onnx: 0.0860s\n",
      "extract_skeletons: 0.0828s\n",
      "predict_segment_onnx: 0.0838s\n",
      "extract_skeletons: 0.0750s\n",
      "predict_segment_onnx: 0.0760s\n",
      "extract_skeletons: 0.0764s\n",
      "predict_segment_onnx: 0.0774s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import onnxruntime as ort\n",
    "from collections import deque\n",
    "import concurrent.futures\n",
    "from rtmlib import Wholebody, draw_skeleton, Body  \n",
    "\n",
    "def extract_skeleton_from_batch(\n",
    "    frames_batch,\n",
    "    skeleton_detector,\n",
    "    num_joints: int = 17,\n",
    "    normalize: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Извлекает ключевые точки для батча кадров и нормализует их.\n",
    "    Возвращает np.ndarray формы (T, num_joints, 2).\n",
    "    \"\"\"\n",
    "    skeletons = []\n",
    "    for frame in frames_batch:\n",
    "        try:\n",
    "            keypoints, scores = skeleton_detector(frame)\n",
    "        except Exception as e:\n",
    "            kp = np.zeros((num_joints, 2), dtype=np.float32)\n",
    "        else:\n",
    "            kp = np.array(keypoints, dtype=np.float32)\n",
    "            if kp.ndim == 3:\n",
    "                kp = kp[0]\n",
    "            # оставляем только xy\n",
    "            if kp.shape[1] > 2:\n",
    "                kp = kp[:, :2]\n",
    "            # приведение числа точек\n",
    "            if kp.shape[0] != num_joints:\n",
    "                if kp.shape[0] < num_joints:\n",
    "                    pad = np.zeros((num_joints - kp.shape[0], 2), dtype=np.float32)\n",
    "                    kp = np.concatenate([kp, pad], axis=0)\n",
    "                else:\n",
    "                    kp = kp[:num_joints]\n",
    "        skeletons.append(kp)\n",
    "\n",
    "    seq = np.stack(skeletons, axis=0)  # (T, V, 2)\n",
    "    if normalize:\n",
    "        mins = seq.min(axis=(0,1))\n",
    "        maxs = seq.max(axis=(0,1))\n",
    "        diff = np.where(maxs - mins == 0, 1.0, maxs - mins)\n",
    "        seq = (seq - mins) / diff\n",
    "    return seq\n",
    "\n",
    "\n",
    "def extract_skeletons_from_frames(\n",
    "    frames,\n",
    "    skeleton_detector,\n",
    "    num_joints=17,\n",
    "    batch_size=16,\n",
    "    normalize=True,\n",
    "    frame_skip=2,\n",
    "    segment_length=8\n",
    ") -> np.ndarray:\n",
    "    start = time.time()\n",
    "    skeletons = []\n",
    "    n = len(frames)\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = frames[i: i + batch_size]\n",
    "        sk = extract_skeleton_from_batch(batch, skeleton_detector, num_joints, normalize)\n",
    "        skeletons.extend(sk)\n",
    "    seq = skeletons[::frame_skip]\n",
    "    # pad or trim\n",
    "    if len(seq) < segment_length:\n",
    "        seq += [seq[-1]] * (segment_length - len(seq))\n",
    "    else:\n",
    "        seq = seq[:segment_length]\n",
    "    print(f\"extract_skeletons: {time.time() - start:.4f}s\")\n",
    "    return np.array(seq, dtype=np.float32)\n",
    "\n",
    "# ===================== ONNX Runtime init =====================\n",
    "onnx_path = \"hybrid_gcn_timesformer_09.onnx\"\n",
    "sess_opts = ort.SessionOptions()\n",
    "sess_opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "providers = ['CPUExecutionProvider']   \n",
    "ort_session = ort.InferenceSession(onnx_path, sess_options=sess_opts, providers=providers)\n",
    "\n",
    "# ===================== Предсказание сегмента через ONNX =====================\n",
    "def predict_segment_onnx(\n",
    "    segment_frames,\n",
    "    skeleton_detector,\n",
    "    ort_session,\n",
    "    segment_length=8,\n",
    "    num_joints=17,\n",
    "    normalize=True,\n",
    "    frame_skip=2,\n",
    "    batch_size=16\n",
    "):\n",
    "    start = time.time()\n",
    "    seq = extract_skeletons_from_frames(\n",
    "        segment_frames, skeleton_detector,\n",
    "        num_joints, batch_size,\n",
    "        normalize, frame_skip,\n",
    "        segment_length\n",
    "    )\n",
    "    x_input = seq[np.newaxis, ...].astype(np.float32)  # (1, T, V, 2)\n",
    "    logits_q, logits_c = ort_session.run(None, {\"x\": x_input})\n",
    "    probs_q = F.softmax(torch.from_numpy(logits_q), dim=1).numpy()\n",
    "    probs_c = F.softmax(torch.from_numpy(logits_c), dim=1).numpy()\n",
    "    label_q = int(np.argmax(probs_q, axis=1)[0])\n",
    "    label_c = int(np.argmax(probs_c, axis=1)[0])\n",
    "    print(f\"predict_segment_onnx: {time.time() - start:.4f}s\")\n",
    "    return label_q, probs_q, label_c, probs_c\n",
    "\n",
    "# ===================== Утилита для внешнего вызова =====================\n",
    "def process_frame_segment_onnx(\n",
    "    segment_frames,\n",
    "    skeleton_detector,\n",
    "    ort_session,\n",
    "    **kwargs\n",
    "):\n",
    "    q, _, c, _ = predict_segment_onnx(\n",
    "        segment_frames, skeleton_detector, ort_session, **kwargs\n",
    "    )\n",
    "    return q, c\n",
    "\n",
    "# ===================== Отображение видео со скользящим окном =====================\n",
    "def display_video_sliding_window_onnx(\n",
    "    video_source,\n",
    "    skeleton_detector,\n",
    "    ort_session,\n",
    "    segment_length=8,\n",
    "    num_joints=17,\n",
    "    normalize=True,\n",
    "    frame_skip=2,\n",
    "    batch_size=16\n",
    "):\n",
    "    \"\"\"\n",
    "    Обрабатывает видео скользящим окном длины segment_length.\n",
    "    Отрисовывает скелет, аннотации, подсчитывает отжимания и показывает FPS.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('output_onnx.avi', fourcc, 30.0, (width, height))\n",
    "\n",
    "    window = deque(maxlen=segment_length)\n",
    "    last_segment_class = None\n",
    "    pushup_count = 0\n",
    "    prev_q = None\n",
    "    prev_c = None\n",
    "    frame_idx = 0\n",
    "    prev_time = time.time()\n",
    "    window_name = \"ONNX Live\"\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        window.append(frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "        # Предсказание по сегменту\n",
    "        if len(window) == segment_length and (frame_idx - segment_length) % frame_skip == 0:\n",
    "            q, c = process_frame_segment_onnx(\n",
    "                list(window),\n",
    "                skeleton_detector,\n",
    "                ort_session,\n",
    "                segment_length=segment_length,\n",
    "                num_joints=num_joints,\n",
    "                normalize=normalize,\n",
    "                frame_skip=frame_skip,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            # print(q, c)\n",
    "            prev_q, prev_c = q, c\n",
    "\n",
    "            # Логика подсчета отжиманий:\n",
    "            # Предполагаем классы count: 1 = верхняя позиция, 2 = нижняя позиция\n",
    "            if q == 2 and last_segment_class == 0 and c == 1:\n",
    "                pushup_count += 1\n",
    "            last_segment_class = c\n",
    "\n",
    "        # Расчет FPS\n",
    "        curr_time = time.time()\n",
    "        fps = 1.0 / (curr_time - prev_time) if curr_time != prev_time else 0.0\n",
    "        prev_time = curr_time\n",
    "\n",
    "        # Детекция и отрисовка скелета\n",
    "        keypoints, scores = skeleton_detector(frame)\n",
    "        frame_result = draw_skeleton(frame, keypoints, scores, kpt_thr=0.5)\n",
    "\n",
    "        # Аннотации на кадре\n",
    "        # if prev_q is not None:\n",
    "        #     # cv2.putText(frame_result, f\"Quality: {prev_q}\", (10, 30),\n",
    "        #     #             cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        #     # cv2.putText(frame_result, f\"Count: {prev_c}\", (10, 60),\n",
    "            #             cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame_result, f\"Push-ups: {pushup_count}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame_result, f\"FPS: {fps:.2f}\", (10, 60),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Показ и запись\n",
    "        cv2.imshow(window_name, frame_result)\n",
    "        out.write(frame_result)\n",
    "\n",
    "        # Выход по 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "video_path =     r\"C:\\Users\\jet\\Desktop\\видосы\\norm_v\\val\\true\\true_0005.mp4\" \n",
    "skeleton_detector = Body(mode='lightweight', backend='onnxruntime', device='cuda')\n",
    "\n",
    "display_video_sliding_window_onnx(\n",
    "    video_path, ## можно передать 0 для захвата видео с веб-камеры.\n",
    "    skeleton_detector,\n",
    "    ort_session,\n",
    "    segment_length=4,\n",
    "    num_joints=17,\n",
    "    normalize=True,\n",
    "    frame_skip=4,\n",
    "    batch_size=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841e8bd-0c2d-4d61-8ef4-70a303cd86b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52799ce-daec-4238-8a6c-d8e9537aea5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
